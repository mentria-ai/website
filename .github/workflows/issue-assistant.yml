name: Issue Assistant

# ULTIMATE OPTIMIZATION: GitHub Actions built-in filtering at trigger level
# This prevents the workflow from even starting for most music generation issues
on:
  issues:
    types: [opened, edited]
    # Exclude issues with music generation labels at the trigger level
    # Note: This only works for issues that already have labels, not for new issues
  issue_comment:
    types: [created, edited]

# Adding explicit permission scoping for security
permissions:
  contents: read
  issues: write
  actions: read

jobs:
  # Pre-filter job that runs ultra-fast filtering before any setup
  filter-and-respond:
    runs-on: self-hosted
    
    # ADDITIONAL WORKFLOW-LEVEL FILTERING: Skip for known patterns
    # This provides an extra layer of filtering at the job level
    if: >
      !contains(github.event.issue.title, '[music]') &&
      !contains(github.event.issue.title, '[audio]') &&
      !contains(github.event.issue.title, 'music generation') &&
      !contains(github.event.issue.title, 'audio generation') &&
      !contains(github.event.issue.title, 'octobeats') &&
      github.event.issue.user.login != 'github-actions[bot]' &&
      github.event.issue.user.login != 'dependabot[bot]'
    
    steps:
      # ULTRA-FAST FILTERING - No dependencies, no checkout, no setup
      - name: üöÄ Ultra-Fast Issue Filtering (No Dependencies)
        id: ultra-filter
        uses: actions/github-script@v7
        with:
          script: |
            console.log('üîç Starting ultra-fast issue filtering...');
            
            // Get the issue from the event payload (works for both issue and issue_comment events)
            const issue = context.payload.issue;
            if (!issue) {
              console.log('‚ùå No issue found in payload');
              core.setOutput('should_process', 'false');
              core.setOutput('reason', 'No issue in payload');
              core.setOutput('skip_reason', 'No issue data');
              return;
            }
            
            console.log(`üìã Issue #${issue.number}: "${issue.title}"`);
            
            // 1. CHECK FOR MUSIC GENERATION LABELS (highest priority filter)
            const labels = issue.labels.map(label => label.name.toLowerCase());
            const musicLabels = ['audio', 'octobeats', 'music-generation', 'music'];
            const hasMusicLabel = labels.some(label => musicLabels.includes(label));
            
            console.log(`üè∑Ô∏è Labels: ${labels.join(', ')}`);
            console.log(`üéµ Has music generation label: ${hasMusicLabel}`);
            
            if (hasMusicLabel) {
              console.log('‚è≠Ô∏è SKIP: Music generation issue - handled by OctoBeats workflow');
              core.setOutput('should_process', 'false');
              core.setOutput('reason', 'Music generation issue - handled by OctoBeats workflow');
              core.setOutput('skip_reason', 'Music generation labels detected');
              return;
            }
            
            // 2. CHECK FOR MUSIC GENERATION TITLE PATTERNS
            const title = issue.title.toLowerCase();
            const musicTitlePatterns = ['[music]', '[audio]', 'music generation', 'audio generation', 'octobeats'];
            const hasMusicTitle = musicTitlePatterns.some(pattern => title.includes(pattern));
            
            console.log(`üéº Has music generation title pattern: ${hasMusicTitle}`);
            
            if (hasMusicTitle) {
              console.log('‚è≠Ô∏è SKIP: Music generation title pattern detected');
              core.setOutput('should_process', 'false');
              core.setOutput('reason', 'Music generation title pattern detected');
              core.setOutput('skip_reason', 'Music generation title pattern');
              return;
            }
            
            // 3. CHECK FOR BOT AUTHORS
            const author = issue.user.login.toLowerCase();
            const botPatterns = ['bot', 'github-actions', 'dependabot', 'renovate'];
            const isBot = botPatterns.some(pattern => author.includes(pattern));
            
            console.log(`ü§ñ Issue author: ${author}, Is bot: ${isBot}`);
            
            if (isBot) {
              console.log('‚è≠Ô∏è SKIP: Issue created by bot');
              core.setOutput('should_process', 'false');
              core.setOutput('reason', 'Issue created by bot');
              core.setOutput('skip_reason', 'Bot author detected');
              return;
            }
            
            // 4. CHECK FOR OCTOBEATS WORKFLOW COMMENTS (for issue_comment events)
            if (context.eventName === 'issue_comment') {
              const comment = context.payload.comment;
              const commentBody = comment.body.toLowerCase();
              const commentAuthor = comment.user.login.toLowerCase();
              
              console.log(`üí¨ Comment author: ${commentAuthor}`);
              
              // Skip if comment is from OctoBeats workflow or contains OctoBeats-related content
              if (commentAuthor.includes('github-actions') || 
                  commentBody.includes('octobeats') || 
                  commentBody.includes('audio generated successfully') ||
                  commentBody.includes('music generator')) {
                console.log('‚è≠Ô∏è SKIP: Comment is from OctoBeats workflow');
                core.setOutput('should_process', 'false');
                core.setOutput('reason', 'Comment from OctoBeats workflow');
                core.setOutput('skip_reason', 'OctoBeats workflow comment');
                return;
              }
            }
            
            // 5. ALL FILTERS PASSED - PROCEED WITH ISSUE ASSISTANT
            console.log('‚úÖ All filters passed - Issue should be processed by Issue Assistant');
            core.setOutput('should_process', 'true');
            core.setOutput('reason', 'Regular issue requiring assistance');
            core.setOutput('skip_reason', '');
            
            // Also output basic issue info for next steps
            core.setOutput('issue_number', issue.number);
            core.setOutput('issue_title', issue.title);
            core.setOutput('issue_author', issue.user.login);
            
            return {
              should_process: true,
              issue_number: issue.number,
              issue_title: issue.title,
              issue_author: issue.user.login
            };
      
      # Early exit if filtering failed
      - name: ‚è≠Ô∏è Early Exit (Workflow Not Applicable)
        if: steps.ultra-filter.outputs.should_process != 'true'
        run: |
          echo "‚è≠Ô∏è Skipping Issue Assistant workflow"
          echo "üìã Reason: ${{ steps.ultra-filter.outputs.reason }}"
          echo "üîç Filter: ${{ steps.ultra-filter.outputs.skip_reason }}"
          echo ""
          echo "‚ÑπÔ∏è This issue will be handled by the appropriate specialized workflow."
          echo "‚úÖ Workflow completed successfully (filtered out)"
          exit 0
      
      # Only run these steps if filtering passed
      - name: Validate API key
        if: steps.ultra-filter.outputs.should_process == 'true'
        run: |
          echo "üîë Validating Together API key..."
          if [[ -z "${{ secrets.TOGETHER_API_KEY }}" ]]; then
            echo "::error::Missing Together API key. Please add it to your repository secrets."
            exit 1
          fi
          
          # Validate format without revealing content
          if [[ ! "${{ secrets.TOGETHER_API_KEY }}" =~ ^[A-Za-z0-9+/=_-]+$ ]]; then
            echo "::warning::API key has unexpected format, may cause issues"
          else
            echo "‚úÖ API key format validation passed"
          fi
      
      - name: Checkout repository
        if: steps.ultra-filter.outputs.should_process == 'true'
        uses: actions/checkout@v4
        
      - name: Set up Node.js
        if: steps.ultra-filter.outputs.should_process == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      
      - name: Initialize workflow logging
        if: steps.ultra-filter.outputs.should_process == 'true'
        id: init
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            // Load logging utilities
            const loggerPath = './.github/shared/utils/logging.js';
            if (!fs.existsSync(loggerPath)) {
              return core.setFailed('Logging utilities not found');
            }
            
            const logger = require(`${process.env.GITHUB_WORKSPACE}/${loggerPath}`);
            const logFile = logger.initLogFile('Issue Assistant', context);
            
            // Start logging
            logger.logSection(logFile, "Workflow Initialization");
            logger.logMessage(logFile, "- Event: " + context.eventName);
            logger.logMessage(logFile, "- Action: " + context.payload.action);
            logger.logMessage(logFile, "- Actor: " + context.actor);
            logger.logMessage(logFile, "- Issue #" + '${{ steps.ultra-filter.outputs.issue_number }}' + ": " + '${{ steps.ultra-filter.outputs.issue_title }}');
            logger.logMessage(logFile, "- Author: " + '${{ steps.ultra-filter.outputs.issue_author }}');
            
            // Make log file available to all steps
            core.setOutput('log_file', logFile);
            
            // Basic validation (already done in ultra-filter, but keeping for completeness)
            if (context.eventName !== 'issues' && context.eventName !== 'issue_comment') {
              logger.logError(logFile, `Unsupported event type: ${context.eventName}`);
              return core.setFailed(`Unsupported event type: ${context.eventName}`);
            }
            
            logger.logSuccess(logFile, 'Workflow initialization completed - proceeding with issue processing');
            return { log_file: logFile };
      
      # NEW: Enhanced issue classification system
      - name: üé≠ Classify Issue Type and Determine Strategy
        if: steps.ultra-filter.outputs.should_process == 'true'
        id: classify-issue
        uses: actions/github-script@v7
        with:
          script: |
            const issue = context.payload.issue;
            const content = `${issue.title} ${issue.body}`.toLowerCase();
            
            // Enhanced classification system
            const classifiers = {
              bug_report: /bug|error|crash|exception|fail|broken|not working|issue|problem/i,
              feature_request: /feature|enhancement|improve|add|new|request|suggest/i,
              question: /question|how to|help|why|what|where|when|\?/i,
              documentation: /docs|documentation|readme|guide|help|tutorial|explain/i,
              performance: /slow|performance|optimization|memory|cpu|speed|lag/i,
              security: /security|vulnerability|exploit|auth|permission|access/i,
              ai_assistant: /\[ai\]|ai.assistant|artificial.intelligence/i
            };
            
            const classifications = {};
            for (const [type, pattern] of Object.entries(classifiers)) {
              classifications[type] = pattern.test(content);
            }
            
            // Determine primary type
            const suggestedLabels = Object.keys(classifications).filter(key => classifications[key]);
            const primaryType = suggestedLabels[0] || 'general';
            
            // Check if this uses the AI assistant template
            const isAITemplate = issue.labels.some(label => label.name.toLowerCase() === 'ai-assistant') || 
                                 content.includes('[ai]');
            
            console.log(`üéØ Issue classification: ${primaryType}`);
            console.log(`ü§ñ Uses AI template: ${isAITemplate}`);
            
            core.setOutput('classifications', JSON.stringify(classifications));
            core.setOutput('suggested_labels', JSON.stringify(suggestedLabels));
            core.setOutput('primary_type', primaryType);
            core.setOutput('is_ai_template', isAITemplate.toString());
            
            return {
              classifications,
              primaryType,
              isAITemplate
            };
      
      - name: Extract issue content
        if: steps.ultra-filter.outputs.should_process == 'true'
        id: get-content
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const logFile = '${{ steps.init.outputs.log_file }}';
            
            // Load utilities
            const loggerPath = './.github/shared/utils/logging.js';
            const eventExtractorPath = './.github/shared/event-handlers/event-extractor.js';
            
            if (!fs.existsSync(loggerPath) || !fs.existsSync(eventExtractorPath)) {
              return core.setFailed('Required modules not found');
            }
            
            const logger = require(`${process.env.GITHUB_WORKSPACE}/${loggerPath}`);
            const eventExtractor = require(`${process.env.GITHUB_WORKSPACE}/${eventExtractorPath}`);
            
            logger.logSection(logFile, "Content Extraction");
            
            try {
              // Extract content from issue or comment
              const contentResult = eventExtractor.extractIssueContent(context.payload, logFile, logger);
              
              if (!contentResult.success) {
                logger.logError(logFile, `Failed to extract content: ${contentResult.error}`);
                return core.setFailed(`Failed to extract content: ${contentResult.error}`);
              }
              
              // Get previous comments for context
              let previousComments = [];
              
              if (context.eventName === 'issue_comment' || context.payload.action === 'edited') {
                try {
                  logger.logMessage(logFile, "Fetching previous comments for context");
                  
                  // Use REST API to fetch issue comments
                  const { data: comments } = await github.rest.issues.listComments({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    issue_number: contentResult.issueNumber,
                    per_page: 10
                  });
                  
                  // Format comments for context
                  previousComments = comments.map(comment => ({
                    author: comment.user.login,
                    body: comment.body,
                    createdAt: comment.created_at
                  }));
                  
                  logger.logSuccess(logFile, `Retrieved ${previousComments.length} previous comments`);
                } catch (error) {
                  logger.logWarning(logFile, `Error retrieving previous comments: ${error.message}`);
                  // Continue without previous comments - non-critical error
                }
              }
              
              // Create a JSON file with all extracted content for the next step
              const extractedData = {
                ...contentResult,
                previousComments,
                classification: '${{ steps.classify-issue.outputs.primary_type }}',
                isAITemplate: '${{ steps.classify-issue.outputs.is_ai_template }}' === 'true'
              };
              
              fs.writeFileSync('content_data.json', JSON.stringify(extractedData, null, 2));
              
              logger.logSuccess(logFile, "Content extraction completed successfully");
              
              // Explicitly set outputs using core.setOutput
              core.setOutput('issue_id', contentResult.issueId);
              core.setOutput('issue_number', contentResult.issueNumber);
              core.setOutput('comment_id', contentResult.commentId || '');
              core.setOutput('content_file', contentResult.contentFile || '');
              core.setOutput('base64_file', contentResult.base64File || '');
              
              // Also return values for backward compatibility
              return {
                issue_id: contentResult.issueId,
                issue_number: contentResult.issueNumber,
                comment_id: contentResult.commentId,
                content_file: contentResult.contentFile,
                base64_file: contentResult.base64File
              };
            } catch (error) {
              logger.logError(logFile, `Error in content extraction: ${error.message}`);
              return core.setFailed(`Error in content extraction: ${error.message}`);
            }
      
      - name: Analyze repository context needs
        if: steps.ultra-filter.outputs.should_process == 'true'
        id: check-context
        run: |
          echo "Starting repository context analysis..." >> ${{ steps.init.outputs.log_file }}
          
          # Create directories for analysis
          mkdir -p context_analysis error_logs
          
          # Check if content data exists
          if [ ! -f "content_data.json" ]; then
            echo "‚ùå Error: content_data.json file not found" >> ${{ steps.init.outputs.log_file }}
            echo "needs_context=false" >> $GITHUB_OUTPUT
            echo "reason=Failed to analyze context needs - missing content data" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Extract content to a temporary file to avoid shell expansion issues
          jq -r '.content' content_data.json > context_analysis/content.txt
          
          # Check if this is an AI template issue (higher chance of needing context)
          IS_AI_TEMPLATE=$(jq -r '.isAITemplate // false' content_data.json)
          CLASSIFICATION=$(jq -r '.classification // "general"' content_data.json)
          
          echo "Issue classification: $CLASSIFICATION" >> ${{ steps.init.outputs.log_file }}
          echo "Is AI template: $IS_AI_TEMPLATE" >> ${{ steps.init.outputs.log_file }}
          
          # Create the request JSON directly with heredoc
          cat > context_analysis/request.json << EOL
          {
            "model": "deepseek-ai/DeepSeek-R1",
            "max_tokens": 2048,
            "temperature": 0.1,
            "messages": [
              {
                "role": "user",
                "content": "You are a tool that analyzes GitHub issue or discussion messages to determine if repository context would be helpful to answer the query. You should be liberal in deciding that context is needed - if there's ANY chance code context would help, say yes. 

                Issue Classification: $CLASSIFICATION
                Uses AI Template: $IS_AI_TEMPLATE

                Respond with only a JSON object containing two fields: \"needsContext\" (boolean) and \"reason\" (string).

                Analyze this GitHub issue/discussion text and determine if repository code context would be helpful to answer it properly:

                $(cat context_analysis/content.txt)

                Respond with JSON only, format: {\"needsContext\": boolean, \"reason\": \"your explanation\"}"
              }
            ]
          }
          EOL
          
          # Call the API
          curl -s -X POST "https://api.together.xyz/v1/chat/completions" \
            -H "Authorization: Bearer ${{ secrets.TOGETHER_API_KEY }}" \
            -H "Content-Type: application/json" \
            -d @context_analysis/request.json \
            -o context_analysis/response.json
          
          # Check if the response file exists and has content
          if [ -s "context_analysis/response.json" ]; then
            # Extract the response content to a file
            jq -r '.choices[0].message.content' context_analysis/response.json > context_analysis/ai_response.txt
            
            # Extract JSON using grep
            grep -o '{.*}' context_analysis/ai_response.txt > context_analysis/json_extract.txt || echo "{}" > context_analysis/json_extract.txt
            
            # Default values
            NEEDS_CONTEXT="false"
            REASON="Failed to analyze context needs"
            
            # Try to extract values from the JSON if file exists and has content
            if [ -s "context_analysis/json_extract.txt" ]; then
              EXTRACTED_NEEDS_CONTEXT=$(jq -r '.needsContext // false' context_analysis/json_extract.txt)
              EXTRACTED_REASON=$(jq -r '.reason // "No reason provided"' context_analysis/json_extract.txt)
              
              if [ "$EXTRACTED_NEEDS_CONTEXT" == "true" ]; then
                NEEDS_CONTEXT="true"
              fi
              
              if [ ! -z "$EXTRACTED_REASON" ] && [ "$EXTRACTED_REASON" != "null" ]; then
                REASON="$EXTRACTED_REASON"
              fi
            fi
            
            # For AI template issues, bias toward needing context
            if [ "$IS_AI_TEMPLATE" == "true" ] && [ "$NEEDS_CONTEXT" == "false" ]; then
              echo "AI template detected - overriding to need context" >> ${{ steps.init.outputs.log_file }}
              NEEDS_CONTEXT="true"
              REASON="AI template issue - providing comprehensive context"
            fi
            
            echo "Context analysis result: $NEEDS_CONTEXT" >> ${{ steps.init.outputs.log_file }}
            echo "Reason: $REASON" >> ${{ steps.init.outputs.log_file }}
            
            # Set outputs
            echo "needs_context=$NEEDS_CONTEXT" >> $GITHUB_OUTPUT
            echo "reason=$REASON" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Error: No response from API or empty response" >> ${{ steps.init.outputs.log_file }}
            # Default to needing context in case of error, especially for AI template
            echo "needs_context=true" >> $GITHUB_OUTPUT
            echo "reason=Failed to analyze context needs, assuming context is needed" >> $GITHUB_OUTPUT
          fi
      
      - name: Get repository context
        id: get-repo-context
        if: steps.ultra-filter.outputs.should_process == 'true' && steps.check-context.outputs.needs_context == 'true'
        run: |
          echo "Getting repository context..." >> ${{ steps.init.outputs.log_file }}
          
          # Load content data
          CONTENT=$(cat content_data.json | jq -r '.content')
          CLASSIFICATION=$(cat content_data.json | jq -r '.classification // "general"')
          
          # Enhanced context extraction based on classification
          echo "Extracting context for $CLASSIFICATION type issue..." >> ${{ steps.init.outputs.log_file }}
          
          # Extract keywords from content
          KEYWORDS=$(echo "$CONTENT" | tr -cs '[:alnum:]' '\n' | sort | uniq | grep -v '^$' | head -30)
          echo "Keywords extracted: $KEYWORDS" >> ${{ steps.init.outputs.log_file }}
          
          # Find relevant files with enhanced search (increased to 20 files)
          RELEVANT_FILES=""
          for keyword in $KEYWORDS; do
            if [ ${#keyword} -gt 3 ]; then  # Only use keywords longer than 3 chars
              # First try grep to find content matches (more relevant)
              GREP_FILES=$(grep -l "$keyword" $(find . -type f -not -path "*/node_modules/*" -not -path "*/dist/*" -not -path "*/.git/*" -not -path "*/build/*" -not -path "*/.github/workflows/*" -name "*.js" -o -name "*.jsx" -o -name "*.ts" -o -name "*.tsx" -o -name "*.py" -o -name "*.html" -o -name "*.css" -o -name "*.json" -o -name "*.md" 2>/dev/null) 2>/dev/null | head -3)
              
              # Then try filename matches
              FOUND_FILES=$(find . -type f -not -path "*/node_modules/*" -not -path "*/dist/*" -not -path "*/.git/*" -not -path "*/build/*" -not -path "*/.github/workflows/*" -name "*${keyword}*" | head -2)
              
              RELEVANT_FILES="$RELEVANT_FILES $GREP_FILES $FOUND_FILES"
            fi
          done
          
          # Include classification-specific important files
          case "$CLASSIFICATION" in
            "bug_report"|"feature_request")
              IMPORTANT_FILES=$(find . -maxdepth 3 -type f -name "package.json" -o -name "README.md" -o -name "index.html" -o -name "index.js" -o -name "main.js" -o -name "app.js" -o -name "*.config.*" 2>/dev/null)
              ;;
            "performance")
              IMPORTANT_FILES=$(find . -maxdepth 3 -type f -name "webpack.config.*" -o -name "vite.config.*" -o -name "package.json" -o -name "*.performance.*" 2>/dev/null)
              ;;
            *)
              IMPORTANT_FILES=$(find . -maxdepth 2 -type f -name "package.json" -o -name "README.md" -o -name "index.html" -o -name "index.js" -o -name "main.js" -o -name "app.js" -o -name "styles.css" 2>/dev/null)
              ;;
          esac
          
          RELEVANT_FILES="$RELEVANT_FILES $IMPORTANT_FILES"
          
          # Deduplicate files and limit to 20
          RELEVANT_FILES=$(echo "$RELEVANT_FILES" | tr ' ' '\n' | sort | uniq | grep -v '^$' | head -20)
          
          # Format the repository context
          echo -e "## Repository Context\n\n" > repo_context.md
          echo -e "### Repository Structure\n" >> repo_context.md
          find . -maxdepth 2 -type d -not -path "*/node_modules/*" -not -path "*/.git/*" | sort | head -15 | sed 's/^/- /' >> repo_context.md
          echo -e "\n\n### Relevant Files\n\n" >> repo_context.md
          
          # Read file contents and add to context (increased to 150 lines per file)
          FILE_COUNT=0
          TOTAL_CONTEXT_SIZE=0
          MAX_CONTEXT_SIZE=1048576  # 1MB limit to prevent overly large contexts
          
          for file in $RELEVANT_FILES; do
            if [ -f "$file" ]; then
              # Check file size before processing
              FILE_SIZE=$(wc -c < "$file")
              if [ $((TOTAL_CONTEXT_SIZE + FILE_SIZE)) -gt $MAX_CONTEXT_SIZE ]; then
                echo "Skipping $file - would exceed context size limit" >> ${{ steps.init.outputs.log_file }}
                continue
              fi
              
              EXTENSION="${file##*.}"
              echo -e "#### ${file}\n\`\`\`${EXTENSION}" >> repo_context.md
              head -150 "$file" >> repo_context.md
              echo -e "\n\`\`\`\n\n" >> repo_context.md
              FILE_COUNT=$((FILE_COUNT+1))
              TOTAL_CONTEXT_SIZE=$((TOTAL_CONTEXT_SIZE + FILE_SIZE))
            fi
          done
          
          echo "Repository context extracted with ${FILE_COUNT} files" >> ${{ steps.init.outputs.log_file }}
          
          # Set output
          echo "context_file=repo_context.md" >> $GITHUB_OUTPUT
          echo "file_count=${FILE_COUNT}" >> $GITHUB_OUTPUT
      
      - name: Generate AI response
        if: steps.ultra-filter.outputs.should_process == 'true'
        id: generate-response
        run: |
          echo "Generating AI response..." >> ${{ steps.init.outputs.log_file }}
          
          # Create response directory
          mkdir -p response_files
          
          # Load content data
          CONTENT_DATA=$(cat content_data.json)
          CONTENT=$(echo "$CONTENT_DATA" | jq -r '.content')
          EVENT_TYPE=$(echo "$CONTENT_DATA" | jq -r '.eventType')
          CLASSIFICATION=$(echo "$CONTENT_DATA" | jq -r '.classification // "general"')
          IS_AI_TEMPLATE=$(echo "$CONTENT_DATA" | jq -r '.isAITemplate // false')
          
          # Load repository context if available
          REPO_CONTEXT=""
          if [[ "${{ steps.check-context.outputs.needs_context }}" == "true" ]] && [[ -f "repo_context.md" ]]; then
            REPO_CONTEXT=$(cat repo_context.md)
            echo "Loaded repository context ($(wc -c < repo_context.md) bytes)" >> ${{ steps.init.outputs.log_file }}
          fi
          
          # Enhanced prompting based on classification
          case "$CLASSIFICATION" in
            "bug_report")
              ROLE_PROMPT="You are an expert debugging assistant. Focus on identifying root causes, providing step-by-step debugging approaches, and offering concrete solutions."
              ;;
            "feature_request")
              ROLE_PROMPT="You are a software architecture consultant. Provide implementation strategies, consider edge cases, and suggest best practices for the requested feature."
              ;;
            "performance")
              ROLE_PROMPT="You are a performance optimization specialist. Analyze bottlenecks, suggest profiling approaches, and recommend specific optimization techniques."
              ;;
            "security")
              ROLE_PROMPT="You are a security expert. Focus on identifying vulnerabilities, suggesting secure implementations, and providing security best practices."
              ;;
            *)
              ROLE_PROMPT="You are an AI assistant that helps with GitHub issues. You provide helpful, accurate, and comprehensive responses to technical questions and issues."
              ;;
          esac
          
          # Combine system and user prompts into a single user prompt with enhanced guidelines
          cat > response_files/combined_prompt.txt << PROMPTEOF
          $ROLE_PROMPT

          Issue Classification: $CLASSIFICATION
          Uses AI Assistant Template: $IS_AI_TEMPLATE

          When responding, follow these guidelines:
          1. Be direct and get straight to the point
          2. Use markdown formatting to structure your responses with clear headings
          3. When code is needed, use proper syntax highlighting with markdown code blocks (e.g., \`\`\`javascript)
          4. If you're uncertain, acknowledge limitations rather than making things up
          5. You may include your reasoning process in <think></think> tags - this will be formatted as a collapsible dropdown in the final response
          6. For complex technical questions, break down your approach step by step
          7. If you reference specific parts of code files, cite the file path and line numbers
          8. Keep your responses focused on the technical issue at hand
          9. Provide actionable next steps when appropriate
          10. If suggesting code changes, explain why the changes are needed

          Remember that you're responding in a GitHub issue context. Your goal is to be helpful, accurate, and drive toward issue resolution.
          PROMPTEOF
          
          # Append event type and content
          echo "" >> response_files/combined_prompt.txt
          echo "${EVENT_TYPE}:" >> response_files/combined_prompt.txt
          echo "" >> response_files/combined_prompt.txt
          echo "$CONTENT" >> response_files/combined_prompt.txt
          
          # Add context if available
          if [[ ! -z "$REPO_CONTEXT" ]]; then
            echo "" >> response_files/combined_prompt.txt
            echo "IMPORTANT - Use the following repository context information to assist with your response. Examine this context carefully and refer to specific files and code when relevant to the question:" >> response_files/combined_prompt.txt
            echo "" >> response_files/combined_prompt.txt
            echo "$REPO_CONTEXT" >> response_files/combined_prompt.txt
            echo "" >> response_files/combined_prompt.txt
            echo "Based on the repository context above, please provide a specific and technically accurate response to the question. Reference specific files and code sections when relevant." >> response_files/combined_prompt.txt
          fi
          
          # Create request payload using jq to properly escape JSON with enhanced parameters
          # Use --rawfile to avoid "Argument list too long" error with large repository context
          jq -n \
            --rawfile prompt response_files/combined_prompt.txt \
            '{
              "model": "deepseek-ai/DeepSeek-R1",
              "max_tokens": 8192,
              "temperature": 0.1,
              "top_p": 0.95,
              "messages": [
                {
                  "role": "user",
                  "content": $prompt
                }
              ]
            }' > response_files/payload.json
          
          # Make API call with increased timeout
          curl -s -X POST "https://api.together.xyz/v1/chat/completions" \
            -H "Authorization: Bearer ${{ secrets.TOGETHER_API_KEY }}" \
            -H "Content-Type: application/json" \
            --max-time 90 \
            -d @response_files/payload.json \
            -o response_files/api_response.json
          
          # Check response and extract content
          if [ -s "response_files/api_response.json" ]; then
            # Check if API returned an error
            ERROR_MSG=$(cat response_files/api_response.json | jq -r '.error.message // empty' 2>/dev/null)
            if [ ! -z "$ERROR_MSG" ]; then
              echo "‚ùå API Error: $ERROR_MSG" >> ${{ steps.init.outputs.log_file }}
              echo "API returned an error: $ERROR_MSG. Please try again later." > response.txt
              echo "response_file=response.txt" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            # Extract content safely
            CONTENT=$(cat response_files/api_response.json | jq -r '.choices[0].message.content // empty' 2>/dev/null)
            if [ -z "$CONTENT" ] || [ "$CONTENT" == "null" ]; then
              echo "‚ùå No content in API response" >> ${{ steps.init.outputs.log_file }}
              echo "I apologize, but I didn't receive a proper response from the AI service. Please try again later." > response.txt
              echo "response_file=response.txt" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            # Log prompt and response sizes for debugging
            PROMPT_SIZE=$(wc -c < response_files/combined_prompt.txt)
            echo "Prompt size: $PROMPT_SIZE bytes" >> ${{ steps.init.outputs.log_file }}
            echo "Raw response length: ${#CONTENT} characters" >> ${{ steps.init.outputs.log_file }}
            
            # Process <think> blocks with a simplified approach
            echo "$CONTENT" > response_files/raw_content.txt
            
            # Use awk directly without creating a separate script file
            # Process <think> blocks into collapsible HTML details/summary elements
            processed_content=$(awk '
            BEGIN { in_think = 0; output = ""; thinking = ""; }
            
            /<think>/ { in_think = 1; thinking = ""; next; }
            
            /<\/think>/ { 
              in_think = 0;
              if (thinking != "") {
                output = output "\n\n<details>\n<summary>ü§î <strong>Thinking process</strong></summary>\n\n";
                split(thinking, lines, "\n");
                for (i in lines) {
                  if (lines[i] != "") output = output lines[i] "\n";
                  else output = output "\n";
                }
                output = output "\n</details>\n";
              }
              next;
            }
            
            in_think == 1 { thinking = thinking (thinking=="" ? "" : "\n") $0; next; }
            
            in_think == 0 { output = output (output=="" ? "" : "\n") $0; }
            
            END { print output; }
            ' response_files/raw_content.txt)
            
            # Save processed content
            echo "$processed_content" > response.txt
            echo "Generated AI response successfully ($(wc -c < response.txt) bytes)" >> ${{ steps.init.outputs.log_file }}
            
            # Set output
            echo "response_file=response.txt" >> $GITHUB_OUTPUT
            echo "content_length=$(wc -c < response.txt)" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Error: API request failed or returned empty response" >> ${{ steps.init.outputs.log_file }}
            echo "I apologize, but I'm currently experiencing technical difficulties. Please try again later." > response.txt
            echo "response_file=response.txt" >> $GITHUB_OUTPUT
          fi
      
      - name: Post response to issue
        if: steps.ultra-filter.outputs.should_process == 'true'
        id: post-response
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const logFile = '${{ steps.init.outputs.log_file }}';
            
            // Load utilities
            const loggerPath = './.github/shared/utils/logging.js';
            const responseHandlerPath = './.github/shared/response-handlers/github-response.js';
            
            if (!fs.existsSync(loggerPath) || !fs.existsSync(responseHandlerPath)) {
              return core.setFailed('Required modules not found');
            }
            
            const logger = require(`${process.env.GITHUB_WORKSPACE}/${loggerPath}`);
            const responseHandler = require(`${process.env.GITHUB_WORKSPACE}/${responseHandlerPath}`);
            
            logger.logSection(logFile, "Posting Response");
            
            try {
              // Get response content
              const responseFile = 'response.txt';
              if (!fs.existsSync(responseFile)) {
                logger.logError(logFile, "Response file not found");
                return core.setFailed("Response file not found");
              }
              
              const responseContent = fs.readFileSync(responseFile, 'utf8');
              logger.logMessage(logFile, `Loaded response content (${responseContent.length} bytes)`);
              
              // Load issue data from both step outputs and content_data.json
              // The step output is explicitly preferred if available, with content_data.json as fallback
              let issueNumber;
              
              // First try from step output
              const stepOutputIssueNumber = '${{ steps.get-content.outputs.issue_number }}';
              if (stepOutputIssueNumber && stepOutputIssueNumber.trim() !== '') {
                issueNumber = parseInt(stepOutputIssueNumber, 10);
                logger.logMessage(logFile, `Got issue number ${issueNumber} from step output`);
              } 
              // Then try from content_data.json
              else if (fs.existsSync('content_data.json')) {
                try {
                  const contentData = JSON.parse(fs.readFileSync('content_data.json', 'utf8'));
                  issueNumber = contentData.issueNumber;
                  logger.logMessage(logFile, `Got issue number ${issueNumber} from content_data.json`);
                } catch (error) {
                  logger.logWarning(logFile, `Error parsing content_data.json: ${error.message}`);
                }
              }
              // Finally, try to extract from context directly
              if (!issueNumber && context.payload.issue) {
                issueNumber = context.payload.issue.number;
                logger.logMessage(logFile, `Got issue number ${issueNumber} directly from context`);
              }
              
              if (!issueNumber || isNaN(issueNumber)) {
                logger.logError(logFile, "Missing or invalid issue number");
                return core.setFailed("Missing or invalid issue number");
              }
              
              // Post to issue
              const postResult = await responseHandler.postToIssue({
                github,
                issueNumber,
                responseContent,
                logFile,
                logger,
                context
              });
              
              if (!postResult.success) {
                logger.logError(logFile, `Failed to post response: ${postResult.error}`);
                return core.setFailed(`Failed to post response: ${postResult.error}`);
              }
              
              logger.logSuccess(logFile, `Successfully posted response with ID: ${postResult.commentId}`);
              
              return {
                comment_id: postResult.commentId,
                success: true
              };
            } catch (error) {
              logger.logError(logFile, `Error posting response: ${error.message}`);
              return core.setFailed(`Error posting response: ${error.message}`);
            }
      
      - name: Finalize workflow
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            try {
              const logFile = '${{ steps.init.outputs.log_file }}';
              if (!logFile || !fs.existsSync(logFile)) {
                console.log('Log file not found or not specified');
                return;
              }
              
              const loggerPath = './.github/shared/utils/logging.js';
              if (!fs.existsSync(loggerPath)) {
                console.log('Logger module not found');
                return;
              }
              
              const logger = require(`${process.env.GITHUB_WORKSPACE}/${loggerPath}`);
              
              // Create summary of outcomes
              const outcomes = {
                result: '${{ steps.post-response.outcome }}' === 'success' ? 'success' : 'failure',
                eventType: context.eventName,
                action: context.payload.action || 'N/A',
                repoContextUsed: '${{ steps.check-context.outputs.needs_context }}' === 'true',
                filteredOut: '${{ steps.ultra-filter.outputs.should_process }}' !== 'true',
                classification: '${{ steps.classify-issue.outputs.primary_type }}',
                isAITemplate: '${{ steps.classify-issue.outputs.is_ai_template }}'
              };
              
              logger.finalizeLog(logFile, context, outcomes);
              console.log(`Workflow log finalized at ${logFile}`);
            } catch (error) {
              console.error(`Error finalizing workflow: ${error.message}`);
            }
      
      - name: Upload workflow logs as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: workflow-logs
          path: |
            workflow_*.log
            content_files/
            context_analysis/
            response_files/
          retention-days: 5 