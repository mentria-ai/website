name: Discussion Assistant

on:
  discussion:
    types: [created]
  discussion_comment:
    types: [created, edited]

# Adding explicit permission scoping for security
permissions:
  contents: read
  discussions: write
  actions: read

jobs:
  respond-to-discussion:
    runs-on: self-hosted
    
    steps:
      - name: Validate API key
        run: |
          echo "Validating Together API key..."
          if [[ -z "${{ secrets.TOGETHER_API_KEY }}" ]]; then
            echo "::error::Missing Together API key. Please add it to your repository secrets."
            exit 1
          fi
          
          # Validate format without revealing content
          if [[ ! "${{ secrets.TOGETHER_API_KEY }}" =~ ^[A-Za-z0-9+/=_-]+$ ]]; then
            echo "::warning::API key has unexpected format, may cause issues"
          else
            echo "API key format validation passed"
          fi
      
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      
      - name: Initialize workflow
        id: init
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            // Load logging utilities
            const loggerPath = './.github/shared/utils/logging.js';
            if (!fs.existsSync(loggerPath)) {
              return core.setFailed('Logging utilities not found');
            }
            
            const logger = require(`${process.env.GITHUB_WORKSPACE}/${loggerPath}`);
            const logFile = logger.initLogFile('Discussion Assistant', context);
            
            // Start logging
            logger.logSection(logFile, "Workflow Initialization");
            logger.logMessage(logFile, "- Event: " + context.eventName);
            logger.logMessage(logFile, "- Action: " + context.payload.action);
            logger.logMessage(logFile, "- Actor: " + context.actor);
            
            // Log payload structure for debugging
            logger.logMessage(logFile, "- Payload Summary:");
            logger.logMessage(logFile, `  - Has discussion: ${!!context.payload.discussion}`);
            logger.logMessage(logFile, `  - Has comment: ${!!context.payload.comment}`);
            if (context.payload.discussion) {
              logger.logMessage(logFile, `  - Discussion properties: ${Object.keys(context.payload.discussion).join(', ')}`);
              logger.logMessage(logFile, `  - Discussion has body: ${!!context.payload.discussion.body}`);
            }
            if (context.payload.comment) {
              logger.logMessage(logFile, `  - Comment properties: ${Object.keys(context.payload.comment).join(', ')}`);
              logger.logMessage(logFile, `  - Comment has body: ${!!context.payload.comment.body}`);
            }
            
            // Make log file available to all steps
            core.setOutput('log_file', logFile);
            
            // Basic validation
            if (context.eventName !== 'discussion' && context.eventName !== 'discussion_comment') {
              logger.logError(logFile, `Unsupported event type: ${context.eventName}`);
              return core.setFailed(`Unsupported event type: ${context.eventName}`);
            }
            
            // Validate event payload
            if (context.eventName === 'discussion' && !context.payload.discussion) {
              logger.logError(logFile, 'Missing discussion data in event payload');
              return core.setFailed('Missing discussion data in event payload');
            }
            
            if (context.eventName === 'discussion_comment') {
              if (!context.payload.discussion) {
                logger.logError(logFile, 'Missing discussion data in comment event payload');
                return core.setFailed('Missing discussion data in comment event payload');
              }
              
              if (!context.payload.comment) {
                logger.logError(logFile, 'Missing comment data in event payload');
                return core.setFailed('Missing comment data in event payload');
              }
            }
            
            logger.logSuccess(logFile, 'Input validation passed');
            return { log_file: logFile };
      
      - name: Extract discussion content
        id: get-content
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const logFile = '${{ steps.init.outputs.log_file }}';
            
            // Load utilities
            const loggerPath = './.github/shared/utils/logging.js';
            const eventExtractorPath = './.github/shared/event-handlers/event-extractor.js';
            
            if (!fs.existsSync(loggerPath) || !fs.existsSync(eventExtractorPath)) {
              return core.setFailed('Required modules not found');
            }
            
            const logger = require(`${process.env.GITHUB_WORKSPACE}/${loggerPath}`);
            const eventExtractor = require(`${process.env.GITHUB_WORKSPACE}/${eventExtractorPath}`);
            
            logger.logSection(logFile, "Content Extraction");
            
            try {
              // Extract content from discussion or comment
              const contentResult = eventExtractor.extractDiscussionContent(context.payload, logFile, logger);
              
              if (!contentResult.success) {
                logger.logError(logFile, `Failed to extract content: ${contentResult.error}`);
                return core.setFailed(`Failed to extract content: ${contentResult.error}`);
              }
              
              // Get previous comments for context
              let previousComments = [];
              
              if (context.eventName === 'discussion_comment') {
                try {
                  logger.logMessage(logFile, "Fetching previous comments for context");
                  
                  // Define GraphQL query for fetching comments
                  const commentsQuery = `
                    query($owner: String!, $repo: String!, $discussionNumber: Int!) {
                      repository(owner: $owner, name: $repo) {
                        discussion(number: $discussionNumber) {
                          title
                          body
                          comments(first: 15) {
                            nodes {
                              author {
                                login
                              }
                              body
                              createdAt
                            }
                          }
                        }
                      }
                    }
                  `;
                  
                  const variables = {
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    discussionNumber: context.payload.discussion.number
                  };
                  
                  logger.logMessage(logFile, `Discussion number: ${context.payload.discussion.number}`);
                  const result = await github.graphql(commentsQuery, variables);
                  
                  // Add discussion title and body for better context
                  const discussionTitle = result.repository.discussion.title;
                  const discussionBody = result.repository.discussion.body;
                  logger.logMessage(logFile, `Discussion title: ${discussionTitle}`);
                  logger.logMessage(logFile, `Found discussion body of ${discussionBody.length} chars`);
                  
                  const comments = result.repository.discussion.comments.nodes;
                  logger.logMessage(logFile, `Retrieved ${comments.length} comments from GraphQL API`);
                  
                  // Add the discussion body as the first "comment" for better context
                  previousComments.push({
                    author: context.payload.discussion.user.login,
                    body: discussionBody,
                    createdAt: context.payload.discussion.created_at,
                    isMainPost: true
                  });
                  
                  // Format comments for context
                  comments.forEach(comment => {
                    // Skip the current comment to avoid duplication
                    if (context.payload.comment && 
                        comment.author.login === context.payload.comment.user.login && 
                        comment.body === context.payload.comment.body) {
                      logger.logMessage(logFile, `Skipping current comment from user ${comment.author.login}`);
                      return;
                    }
                    
                    previousComments.push({
                      author: comment.author.login,
                      body: comment.body,
                      createdAt: comment.createdAt
                    });
                  });
                  
                  logger.logSuccess(logFile, `Total context items (discussion + comments): ${previousComments.length}`);
                  logger.logMessage(logFile, `First comment from: ${previousComments[0]?.author || 'none'}`);
                  logger.logMessage(logFile, `Last comment from: ${previousComments[previousComments.length-1]?.author || 'none'}`);
                } catch (error) {
                  logger.logWarning(logFile, `Error retrieving previous comments: ${error.message}`);
                  logger.logWarning(logFile, `Stack trace: ${error.stack}`);
                  // Continue without previous comments - non-critical error
                }
              } else if (context.eventName === 'discussion') {
                // For new discussions, just log that no previous comments exist
                logger.logMessage(logFile, "New discussion - no previous comments to retrieve");
              }
              
              // Create a JSON file with all extracted content for the next step
              const extractedData = {
                ...contentResult,
                previousComments
              };
              
              fs.writeFileSync('content_data.json', JSON.stringify(extractedData, null, 2));
              
              logger.logSuccess(logFile, "Content extraction completed successfully");
              
              return {
                discussion_id: contentResult.discussionId,
                comment_id: contentResult.commentId,
                content_file: contentResult.contentFile,
                base64_file: contentResult.base64File
              };
            } catch (error) {
              logger.logError(logFile, `Error in content extraction: ${error.message}`);
              return core.setFailed(`Error in content extraction: ${error.message}`);
            }
      
      - name: Analyze repository context needs
        id: check-context
        run: |
          echo "Starting repository context analysis..." >> ${{ steps.init.outputs.log_file }}
          
          # Create directories for analysis
          mkdir -p context_analysis error_logs
          
          # Check if content data exists
          if [ ! -f "content_data.json" ]; then
            echo "❌ Error: content_data.json file not found" >> ${{ steps.init.outputs.log_file }}
            echo "needs_context=false" >> $GITHUB_OUTPUT
            echo "reason=Failed to analyze context needs - missing content data" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Extract content to a temporary file to avoid shell expansion issues
          jq -r '.content' content_data.json > context_analysis/content.txt
          
          # Create the request JSON directly with heredoc
          cat > context_analysis/request.json << EOL
          {
            "model": "deepseek-ai/DeepSeek-R1",
            "messages": [
              {
                "role": "user",
                "content": "You are a tool that analyzes GitHub issue or discussion messages to determine if repository context would be helpful to answer the query. You should be liberal in deciding that context is needed - if there's ANY chance code context would help, say yes. Respond with only a JSON object containing two fields: \"needsContext\" (boolean) and \"reason\" (string).\n\nAnalyze this GitHub issue/discussion text and determine if repository code context would be helpful to answer it properly:\n\n$(cat context_analysis/content.txt)\n\nRespond with JSON only, format: {\"needsContext\": boolean, \"reason\": \"your explanation\"}"
              }
            ]
          }
          EOL
          
          # Call the API
          curl -s -X POST "https://api.together.xyz/v1/chat/completions" \
            -H "Authorization: Bearer ${{ secrets.TOGETHER_API_KEY }}" \
            -H "Content-Type: application/json" \
            -d @context_analysis/request.json \
            -o context_analysis/response.json
          
          # Check if the response file exists and has content
          if [ -s "context_analysis/response.json" ]; then
            # Extract the response content to a file
            jq -r '.choices[0].message.content' context_analysis/response.json > context_analysis/ai_response.txt
            
            # Extract JSON using grep
            grep -o '{.*}' context_analysis/ai_response.txt > context_analysis/json_extract.txt || echo "{}" > context_analysis/json_extract.txt
            
            # Default values
            NEEDS_CONTEXT="false"
            REASON="Failed to analyze context needs"
            
            # Try to extract values from the JSON if file exists and has content
            if [ -s "context_analysis/json_extract.txt" ]; then
              EXTRACTED_NEEDS_CONTEXT=$(jq -r '.needsContext // false' context_analysis/json_extract.txt)
              EXTRACTED_REASON=$(jq -r '.reason // "No reason provided"' context_analysis/json_extract.txt)
              
              if [ "$EXTRACTED_NEEDS_CONTEXT" == "true" ]; then
                NEEDS_CONTEXT="true"
              fi
              
              if [ ! -z "$EXTRACTED_REASON" ] && [ "$EXTRACTED_REASON" != "null" ]; then
                REASON="$EXTRACTED_REASON"
              fi
            fi
            
            echo "Context analysis result: $NEEDS_CONTEXT" >> ${{ steps.init.outputs.log_file }}
            echo "Reason: $REASON" >> ${{ steps.init.outputs.log_file }}
            
            # Set outputs
            echo "needs_context=$NEEDS_CONTEXT" >> $GITHUB_OUTPUT
            echo "reason=$REASON" >> $GITHUB_OUTPUT
          else
            echo "❌ Error: No response from API or empty response" >> ${{ steps.init.outputs.log_file }}
            # Default to needing context in case of error
            echo "needs_context=true" >> $GITHUB_OUTPUT
            echo "reason=Failed to analyze context needs, assuming context is needed" >> $GITHUB_OUTPUT
          fi
      
      - name: Get repository context
        id: get-repo-context
        if: steps.check-context.outputs.needs_context == 'true'
        run: |
          echo "Getting repository context..." >> ${{ steps.init.outputs.log_file }}
          
          # Load content data
          CONTENT=$(cat content_data.json | jq -r '.content')
          
          # We'll use a simpler approach for repo context in the shell script
          # Just identify some relevant files based on keywords in the content
          
          # Extract keywords from content
          KEYWORDS=$(echo "$CONTENT" | tr -cs '[:alnum:]' '\n' | sort | uniq | grep -v '^$' | head -20)
          echo "Keywords extracted: $KEYWORDS" >> ${{ steps.init.outputs.log_file }}
          
          # Find relevant files (limited to 15)
          RELEVANT_FILES=""
          for keyword in $KEYWORDS; do
            if [ ${#keyword} -gt 3 ]; then  # Only use keywords longer than 3 chars
              # First try grep to find content matches (more relevant)
              GREP_FILES=$(grep -l "$keyword" $(find . -type f -not -path "*/node_modules/*" -not -path "*/dist/*" -not -path "*/.git/*" -not -path "*/build/*" -not -path "*/.github/workflows/*" -name "*.js" -o -name "*.jsx" -o -name "*.ts" -o -name "*.tsx" -o -name "*.py" -o -name "*.html" -o -name "*.css" 2>/dev/null) 2>/dev/null | head -3)
              
              # Then try filename matches
              FOUND_FILES=$(find . -type f -not -path "*/node_modules/*" -not -path "*/dist/*" -not -path "*/.git/*" -not -path "*/build/*" -not -path "*/.github/workflows/*" -name "*${keyword}*" | head -2)
              
              RELEVANT_FILES="$RELEVANT_FILES $GREP_FILES $FOUND_FILES"
            fi
          done
          
          # Also include obviously important files
          IMPORTANT_FILES=$(find . -maxdepth 2 -type f -name "package.json" -o -name "README.md" -o -name "index.html" -o -name "index.js" -o -name "main.js" -o -name "app.js" -o -name "styles.css" 2>/dev/null)
          RELEVANT_FILES="$RELEVANT_FILES $IMPORTANT_FILES"
          
          # Deduplicate files and limit to 15
          RELEVANT_FILES=$(echo "$RELEVANT_FILES" | tr ' ' '\n' | sort | uniq | grep -v '^$' | head -15)
          
          # Format the repository context
          echo -e "## Repository Context\n\n" > repo_context.md
          echo -e "### Repository Structure\n" >> repo_context.md
          find . -maxdepth 2 -type d -not -path "*/node_modules/*" -not -path "*/.git/*" | sort | head -10 | sed 's/^/- /' >> repo_context.md
          echo -e "\n\n### Relevant Files\n\n" >> repo_context.md
          
          # Read file contents and add to context
          FILE_COUNT=0
          for file in $RELEVANT_FILES; do
            if [ -f "$file" ]; then
              EXTENSION="${file##*.}"
              echo -e "#### ${file}\n\`\`\`${EXTENSION}" >> repo_context.md
              head -100 "$file" >> repo_context.md
              echo -e "\n\`\`\`\n\n" >> repo_context.md
              FILE_COUNT=$((FILE_COUNT+1))
            fi
          done
          
          echo "Repository context extracted with ${FILE_COUNT} files" >> ${{ steps.init.outputs.log_file }}
          
          # Set output
          echo "context_file=repo_context.md" >> $GITHUB_OUTPUT
          echo "file_count=${FILE_COUNT}" >> $GITHUB_OUTPUT
      
      - name: Generate AI response
        id: generate-response
        run: |
          echo "Generating AI response..." >> ${{ steps.init.outputs.log_file }}
          
          # Create response directory
          mkdir -p response_files
          
          # Load content data
          CONTENT_DATA=$(cat content_data.json)
          CONTENT=$(echo "$CONTENT_DATA" | jq -r '.content')
          EVENT_TYPE=$(echo "$CONTENT_DATA" | jq -r '.eventType')
          
          # Extract previous comments for context
          PREVIOUS_COMMENTS=$(echo "$CONTENT_DATA" | jq -r '.previousComments')
          CONVERSATION_CONTEXT=""
          
          # Format previous comments if they exist and aren't empty array
          if [[ "$PREVIOUS_COMMENTS" != "[]" && "$PREVIOUS_COMMENTS" != "null" ]]; then
            echo "Processing previous comments for context..." >> ${{ steps.init.outputs.log_file }}
            CONVERSATION_CONTEXT="### Previous Discussion Context\n\n"
            
            # Extract each comment and format it
            COMMENT_COUNT=$(echo "$CONTENT_DATA" | jq '.previousComments | length')
            echo "Found $COMMENT_COUNT previous comments/context items to process" >> ${{ steps.init.outputs.log_file }}
            
            # Debug raw comments object (truncated)
            COMMENTS_EXCERPT=$(echo "$PREVIOUS_COMMENTS" | head -c 1000)
            echo "Comments data excerpt: $COMMENTS_EXCERPT" >> ${{ steps.init.outputs.log_file }}
            
            for i in $(seq 0 $(($COMMENT_COUNT - 1))); do
              AUTHOR=$(echo "$CONTENT_DATA" | jq -r ".previousComments[$i].author")
              IS_MAIN_POST=$(echo "$CONTENT_DATA" | jq -r ".previousComments[$i].isMainPost // false")
              BODY=$(echo "$CONTENT_DATA" | jq -r ".previousComments[$i].body")
              CREATED_AT=$(echo "$CONTENT_DATA" | jq -r ".previousComments[$i].createdAt")
              
              # Log the processing of each comment
              BODY_EXCERPT=$(echo "$BODY" | head -c 100 | tr -d '\n')
              echo "Processing comment $i from $AUTHOR (${#BODY} chars): $BODY_EXCERPT..." >> ${{ steps.init.outputs.log_file }}
              
              # Format timestamp to be more readable
              FORMATTED_DATE=$(date -d "$CREATED_AT" "+%Y-%m-%d %H:%M" 2>/dev/null || echo "$CREATED_AT")
              
              # Add to conversation context with special handling for the main post
              if [[ "$IS_MAIN_POST" == "true" ]]; then
                CONVERSATION_CONTEXT="${CONVERSATION_CONTEXT}**Original Post by @${AUTHOR}** (${FORMATTED_DATE}):\n${BODY}\n\n---\n\n"
              else
                CONVERSATION_CONTEXT="${CONVERSATION_CONTEXT}**@${AUTHOR}** (${FORMATTED_DATE}):\n${BODY}\n\n---\n\n"
              fi
            done
            
            # Log some stats about the conversation context
            CONTEXT_SIZE=${#CONVERSATION_CONTEXT}
            echo "Added $COMMENT_COUNT items to conversation context. Total size: $CONTEXT_SIZE chars" >> ${{ steps.init.outputs.log_file }}
            
            # Log a sample of the generated conversation context
            CONTEXT_EXCERPT=$(echo -e "$CONVERSATION_CONTEXT" | head -c 500 | tr -d '\n')
            echo "Conversation context sample: $CONTEXT_EXCERPT..." >> ${{ steps.init.outputs.log_file }}
          else
            echo "No previous comments found or invalid format: $PREVIOUS_COMMENTS" >> ${{ steps.init.outputs.log_file }}
          fi
          
          # Load repository context if available
          REPO_CONTEXT=""
          if [[ "${{ steps.check-context.outputs.needs_context }}" == "true" ]] && [[ -f "repo_context.md" ]]; then
            REPO_CONTEXT=$(cat repo_context.md)
            echo "Loaded repository context ($(wc -c < repo_context.md) bytes)" >> ${{ steps.init.outputs.log_file }}
          fi
          
          # Combine system and user prompts into a single user prompt
          cat > response_files/combined_prompt.txt << 'PROMPTEOF'
          You are an AI assistant that helps with GitHub discussions. You provide helpful, accurate, and concise responses to technical questions and discussions.

          When responding, follow these guidelines:
          1. Be direct and get straight to the point
          2. Use markdown formatting to structure your responses
          3. When code is needed, use proper syntax highlighting with markdown code blocks (e.g., ```javascript)
          4. If you're uncertain, acknowledge limitations rather than making things up
          5. You may include your reasoning process in <think></think> tags - this will be formatted as a quote block in the final response
          6. For complex technical questions, break down your approach step by step
          7. If you reference specific parts of code files, cite the file path and line numbers
          8. Keep your responses focused on the technical issue at hand

          Remember that you're responding in a GitHub discussion context. Your goal is to be helpful, accurate, and drive toward resolution.
          PROMPTEOF
          
          # Append event type and content
          echo "" >> response_files/combined_prompt.txt
          
          # Add conversation context if available
          if [[ ! -z "$CONVERSATION_CONTEXT" ]]; then
            echo "Adding conversation context to prompt" >> ${{ steps.init.outputs.log_file }}
            echo -e "$CONVERSATION_CONTEXT" >> response_files/combined_prompt.txt
            echo "Current message to respond to:" >> response_files/combined_prompt.txt
            echo "" >> response_files/combined_prompt.txt
          else
            echo "No conversation context available to add to prompt" >> ${{ steps.init.outputs.log_file }}
          fi
          
          echo "${EVENT_TYPE}:" >> response_files/combined_prompt.txt
          echo "" >> response_files/combined_prompt.txt
          echo "$CONTENT" >> response_files/combined_prompt.txt
          
          # Add context if available
          if [[ ! -z "$REPO_CONTEXT" ]]; then
            echo "Adding repository context to prompt ($(wc -c <<< "$REPO_CONTEXT") bytes)" >> ${{ steps.init.outputs.log_file }}
            echo "" >> response_files/combined_prompt.txt
            echo "IMPORTANT - Use the following repository context information to assist with your response. Examine this context carefully and refer to specific files and code when relevant to the question:" >> response_files/combined_prompt.txt
            echo "" >> response_files/combined_prompt.txt
            echo "$REPO_CONTEXT" >> response_files/combined_prompt.txt
            echo "" >> response_files/combined_prompt.txt
            echo "Based on the repository context above, please provide a specific and technically accurate response to the question. Reference specific files and code sections when relevant." >> response_files/combined_prompt.txt
          else
            echo "No repository context available to add to prompt" >> ${{ steps.init.outputs.log_file }}
          fi
          
          # Log the size of the combined prompt
          PROMPT_SIZE=$(wc -c < response_files/combined_prompt.txt)
          echo "Combined prompt size: $PROMPT_SIZE bytes" >> ${{ steps.init.outputs.log_file }}
          
          # Log a sample of the final prompt
          echo "Prompt sample (first 1000 chars):" >> ${{ steps.init.outputs.log_file }}
          head -c 1000 response_files/combined_prompt.txt >> ${{ steps.init.outputs.log_file }}
          echo "..." >> ${{ steps.init.outputs.log_file }}
          
          # Create request payload using jq to properly escape JSON
          jq -n \
            --arg prompt "$(cat response_files/combined_prompt.txt)" \
            '{
              "model": "deepseek-ai/DeepSeek-R1",
              "messages": [
                {
                  "role": "user",
                  "content": $prompt
                }
              ]
            }' > response_files/payload.json
          
          # Make API call
          echo "Making API call to Together AI (DeepSeek-R1)..." >> ${{ steps.init.outputs.log_file }}
          curl -s -X POST "https://api.together.xyz/v1/chat/completions" \
            -H "Authorization: Bearer ${{ secrets.TOGETHER_API_KEY }}" \
            -H "Content-Type: application/json" \
            -d @response_files/payload.json \
            -o response_files/api_response.json
          
          # Check response and extract content
          if [ -s "response_files/api_response.json" ]; then
            # Check if the API response has an error
            API_ERROR=$(jq -r '.error.message // empty' response_files/api_response.json)
            if [[ ! -z "$API_ERROR" ]]; then
              echo "❌ API Error: $API_ERROR" >> ${{ steps.init.outputs.log_file }}
              echo "I apologize, but I encountered an error while processing your request. Please try again later." > response.txt
              echo "response_file=response.txt" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            # Log API response details
            RESPONSE_SIZE=$(wc -c < response_files/api_response.json)
            echo "Received API response ($RESPONSE_SIZE bytes)" >> ${{ steps.init.outputs.log_file }}
            
            # Extract model information
            MODEL=$(jq -r '.model // "unknown"' response_files/api_response.json)
            USAGE=$(jq -r '.usage // "unknown"' response_files/api_response.json)
            echo "Model: $MODEL, Usage: $USAGE" >> ${{ steps.init.outputs.log_file }}
            
            # Extract content
            CONTENT=$(cat response_files/api_response.json | jq -r '.choices[0].message.content')
            CONTENT_LENGTH=${#CONTENT}
            echo "Extracted raw content from API response ($CONTENT_LENGTH chars)" >> ${{ steps.init.outputs.log_file }}
            
            # Save a sample of the raw content for debugging
            echo "Raw content sample (first 500 chars):" >> ${{ steps.init.outputs.log_file }}
            echo "$CONTENT" | head -c 500 >> ${{ steps.init.outputs.log_file }}
            echo "..." >> ${{ steps.init.outputs.log_file }}
            
            # Process <think> blocks with a simplified approach
            echo "$CONTENT" > response_files/raw_content.txt
            
            # Check for think tags in response
            if grep -q "<think>" response_files/raw_content.txt; then
              echo "Found <think> tags in response, processing..." >> ${{ steps.init.outputs.log_file }}
            else
              echo "No <think> tags found in response" >> ${{ steps.init.outputs.log_file }}
            fi
            
            # Use awk directly without creating a separate script file
            processed_content=$(awk '
            BEGIN { in_think = 0; output = ""; thinking = ""; }
            
            /<think>/ { in_think = 1; thinking = ""; next; }
            
            /<\/think>/ { 
              in_think = 0;
              if (thinking != "") {
                output = output "\n\n> **Thinking process:**\n";
                split(thinking, lines, "\n");
                for (i in lines) {
                  if (lines[i] != "") output = output "> " lines[i] "\n";
                  else output = output ">\n";
                }
                output = output "\n";
              }
              next;
            }
            
            in_think == 1 { thinking = thinking (thinking=="" ? "" : "\n") $0; next; }
            
            in_think == 0 { output = output (output=="" ? "" : "\n") $0; }
            
            END { print output; }
            ' response_files/raw_content.txt)
            
            # Save processed content
            echo "$processed_content" > response.txt
            FINAL_SIZE=$(wc -c < response.txt)
            echo "Generated AI response successfully (${FINAL_SIZE} bytes)" >> ${{ steps.init.outputs.log_file }}
            
            # Log a sample of the final response
            echo "Final response sample (first 500 chars):" >> ${{ steps.init.outputs.log_file }}
            head -c 500 response.txt >> ${{ steps.init.outputs.log_file }}
            echo "..." >> ${{ steps.init.outputs.log_file }}
            
            # Set output
            echo "response_file=response.txt" >> $GITHUB_OUTPUT
            echo "content_length=$FINAL_SIZE" >> $GITHUB_OUTPUT
          else
            echo "❌ Error: API request failed or returned empty response" >> ${{ steps.init.outputs.log_file }}
            echo "I apologize, but I'm currently experiencing technical difficulties. Please try again later." > response.txt
            echo "response_file=response.txt" >> $GITHUB_OUTPUT
          fi
      
      - name: Post response to discussion
        id: post-response
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const logFile = '${{ steps.init.outputs.log_file }}';
            
            // Load utilities
            const loggerPath = './.github/shared/utils/logging.js';
            const responseHandlerPath = './.github/shared/response-handlers/github-response.js';
            
            if (!fs.existsSync(loggerPath) || !fs.existsSync(responseHandlerPath)) {
              return core.setFailed('Required modules not found');
            }
            
            const logger = require(`${process.env.GITHUB_WORKSPACE}/${loggerPath}`);
            const responseHandler = require(`${process.env.GITHUB_WORKSPACE}/${responseHandlerPath}`);
            
            logger.logSection(logFile, "Posting Response");
            
            try {
              // Get response content
              const responseFile = 'response.txt';
              if (!fs.existsSync(responseFile)) {
                logger.logError(logFile, "Response file not found");
                return core.setFailed("Response file not found");
              }
              
              const responseContent = fs.readFileSync(responseFile, 'utf8');
              logger.logMessage(logFile, `Loaded response content (${responseContent.length} bytes)`);
              
              // Load discussion data from content_data.json to handle potential missing outputs
              let discussionId, commentId;
              
              if (fs.existsSync('content_data.json')) {
                const contentData = JSON.parse(fs.readFileSync('content_data.json', 'utf8'));
                discussionId = contentData.discussionId;
                commentId = contentData.commentId;
                logger.logMessage(logFile, `Got discussion ID ${discussionId} from content_data.json`);
                
                // Log comment ID if available
                if (commentId) {
                  logger.logMessage(logFile, `Got comment ID ${commentId} from content_data.json`);
                } else {
                  logger.logMessage(logFile, `No comment ID found in content_data.json - posting to main discussion`);
                }
                
                // Log the number of previous comments
                if (contentData.previousComments && Array.isArray(contentData.previousComments)) {
                  logger.logMessage(logFile, `Content data contains ${contentData.previousComments.length} previous comments`);
                  
                  // Log the first few comment authors
                  if (contentData.previousComments.length > 0) {
                    const authors = contentData.previousComments
                      .slice(0, Math.min(3, contentData.previousComments.length))
                      .map(c => c.author)
                      .join(', ');
                    logger.logMessage(logFile, `First few comment authors: ${authors}`);
                  }
                } else {
                  logger.logMessage(logFile, `No previous comments array found in content_data.json`);
                }
              } else {
                // Fallback to step outputs
                discussionId = '${{ steps.get-content.outputs.discussion_id }}';
                commentId = '${{ steps.get-content.outputs.comment_id }}' || null;
                logger.logMessage(logFile, `Got discussion ID ${discussionId} from step output`);
                logger.logMessage(logFile, `Got comment ID ${commentId || 'none'} from step output`);
              }
              
              if (!discussionId) {
                logger.logError(logFile, "Missing discussion ID");
                return core.setFailed("Missing discussion ID");
              }
              
              // Post to discussion
              const postResult = await responseHandler.postToDiscussion({
                github,
                discussionId,
                commentId,
                responseContent,
                logFile,
                logger,
                context
              });
              
              if (!postResult.success) {
                logger.logError(logFile, `Failed to post response: ${postResult.error}`);
                return core.setFailed(`Failed to post response: ${postResult.error}`);
              }
              
              logger.logSuccess(logFile, `Successfully posted response with ID: ${postResult.commentId}`);
              
              return {
                comment_id: postResult.commentId,
                success: true
              };
            } catch (error) {
              logger.logError(logFile, `Error posting response: ${error.message}`);
              return core.setFailed(`Error posting response: ${error.message}`);
            }
      
      - name: Finalize workflow
        if: always()
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            try {
              const logFile = '${{ steps.init.outputs.log_file }}';
              if (!logFile || !fs.existsSync(logFile)) {
                console.log('Log file not found or not specified');
                return;
              }
              
              const loggerPath = './.github/shared/utils/logging.js';
              if (!fs.existsSync(loggerPath)) {
                console.log('Logger module not found');
                return;
              }
              
              const logger = require(`${process.env.GITHUB_WORKSPACE}/${loggerPath}`);
              
              // Create summary of outcomes
              const outcomes = {
                result: '${{ steps.post-response.outcome }}' === 'success' ? 'success' : 'failure',
                eventType: context.eventName,
                action: context.payload.action || 'N/A',
                repoContextUsed: '${{ steps.check-context.outputs.needs_context }}' === 'true'
              };
              
              logger.finalizeLog(logFile, context, outcomes);
              console.log(`Workflow log finalized at ${logFile}`);
            } catch (error) {
              console.error(`Error finalizing workflow: ${error.message}`);
            }
      
      - name: Upload workflow logs as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: workflow-logs
          path: |
            workflow_*.log
            content_files/
            context_analysis/
            response_files/
          retention-days: 5 