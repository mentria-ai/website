name: Issue Assistant

# ULTIMATE OPTIMIZATION: GitHub Actions built-in filtering at trigger level
# This prevents the workflow from even starting for most music generation issues
on:
  issues:
    types: [opened, edited]
    # Exclude issues with music generation labels at the trigger level
    # Note: This only works for issues that already have labels, not for new issues
  issue_comment:
    types: [created, edited]

# Adding explicit permission scoping for security
permissions:
  contents: read
  issues: write
  actions: read

jobs:
  # Pre-filter job that runs ultra-fast filtering before any setup
  filter-and-respond:
    runs-on: self-hosted
    
    # ADDITIONAL WORKFLOW-LEVEL FILTERING: Skip for known patterns
    # This provides an extra layer of filtering at the job level
    if: >
      !contains(github.event.issue.title, '[music]') &&
      !contains(github.event.issue.title, '[audio]') &&
      !contains(github.event.issue.title, 'music generation') &&
      !contains(github.event.issue.title, 'audio generation') &&
      !contains(github.event.issue.title, 'octobeats') &&
      github.event.issue.user.login != 'github-actions[bot]' &&
      github.event.issue.user.login != 'dependabot[bot]'
    
    steps:
      # ULTRA-FAST FILTERING - No dependencies, no checkout, no setup
      - name: ðŸš€ Ultra-Fast Issue Filtering (No Dependencies)
        id: ultra-filter
        uses: actions/github-script@v7
        with:
          script: |
            console.log('ðŸ” Starting ultra-fast issue filtering...');
            
            // Get the issue from the event payload (works for both issue and issue_comment events)
            const issue = context.payload.issue;
            if (!issue) {
              console.log('âŒ No issue found in payload');
              core.setOutput('should_process', 'false');
              core.setOutput('reason', 'No issue in payload');
              core.setOutput('skip_reason', 'No issue data');
              return;
            }
            
            console.log(`ðŸ“‹ Issue #${issue.number}: "${issue.title}"`);
            
            // 1. CHECK FOR MUSIC GENERATION LABELS (highest priority filter)
            const labels = issue.labels.map(label => label.name.toLowerCase());
            const musicLabels = ['audio', 'octobeats', 'music-generation', 'music'];
            const hasMusicLabel = labels.some(label => musicLabels.includes(label));
            
            console.log(`ðŸ·ï¸ Labels: ${labels.join(', ')}`);
            console.log(`ðŸŽµ Has music generation label: ${hasMusicLabel}`);
            
            if (hasMusicLabel) {
              console.log('â­ï¸ SKIP: Music generation issue - handled by OctoBeats workflow');
              core.setOutput('should_process', 'false');
              core.setOutput('reason', 'Music generation issue - handled by OctoBeats workflow');
              core.setOutput('skip_reason', 'Music generation labels detected');
              return;
            }
            
            // 2. CHECK FOR MUSIC GENERATION TITLE PATTERNS
            const title = issue.title.toLowerCase();
            const musicTitlePatterns = ['[music]', '[audio]', 'music generation', 'audio generation', 'octobeats'];
            const hasMusicTitle = musicTitlePatterns.some(pattern => title.includes(pattern));
            
            console.log(`ðŸŽ¼ Has music generation title pattern: ${hasMusicTitle}`);
            
            if (hasMusicTitle) {
              console.log('â­ï¸ SKIP: Music generation title pattern detected');
              core.setOutput('should_process', 'false');
              core.setOutput('reason', 'Music generation title pattern detected');
              core.setOutput('skip_reason', 'Music generation title pattern');
              return;
            }
            
            // 3. CHECK FOR BOT AUTHORS
            const author = issue.user.login.toLowerCase();
            const botPatterns = ['bot', 'github-actions', 'dependabot', 'renovate'];
            const isBot = botPatterns.some(pattern => author.includes(pattern));
            
            console.log(`ðŸ¤– Issue author: ${author}, Is bot: ${isBot}`);
            
            if (isBot) {
              console.log('â­ï¸ SKIP: Issue created by bot');
              core.setOutput('should_process', 'false');
              core.setOutput('reason', 'Issue created by bot');
              core.setOutput('skip_reason', 'Bot author detected');
              return;
            }
            
            // 4. CHECK FOR OCTOBEATS WORKFLOW COMMENTS (for issue_comment events)
            if (context.eventName === 'issue_comment') {
              const comment = context.payload.comment;
              const commentBody = comment.body.toLowerCase();
              const commentAuthor = comment.user.login.toLowerCase();
              
              console.log(`ðŸ’¬ Comment author: ${commentAuthor}`);
              
              // Skip if comment is from OctoBeats workflow or contains OctoBeats-related content
              if (commentAuthor.includes('github-actions') || 
                  commentBody.includes('octobeats') || 
                  commentBody.includes('audio generated successfully') ||
                  commentBody.includes('music generator')) {
                console.log('â­ï¸ SKIP: Comment is from OctoBeats workflow');
                core.setOutput('should_process', 'false');
                core.setOutput('reason', 'Comment from OctoBeats workflow');
                core.setOutput('skip_reason', 'OctoBeats workflow comment');
                return;
              }
            }
            
            // 5. ALL FILTERS PASSED - PROCEED WITH ISSUE ASSISTANT
            console.log('âœ… All filters passed - Issue should be processed by Issue Assistant');
            core.setOutput('should_process', 'true');
            core.setOutput('reason', 'Regular issue requiring assistance');
            core.setOutput('skip_reason', '');
            
            // Also output basic issue info for next steps
            core.setOutput('issue_number', issue.number);
            core.setOutput('issue_title', issue.title);
            core.setOutput('issue_author', issue.user.login);
            
            return {
              should_process: true,
              issue_number: issue.number,
              issue_title: issue.title,
              issue_author: issue.user.login
            };
      
      # Early exit if filtering failed
      - name: â­ï¸ Early Exit (Workflow Not Applicable)
        if: steps.ultra-filter.outputs.should_process != 'true'
        run: |
          echo "â­ï¸ Skipping Issue Assistant workflow"
          echo "ðŸ“‹ Reason: ${{ steps.ultra-filter.outputs.reason }}"
          echo "ðŸ” Filter: ${{ steps.ultra-filter.outputs.skip_reason }}"
          echo ""
          echo "â„¹ï¸ This issue will be handled by the appropriate specialized workflow."
          echo "âœ… Workflow completed successfully (filtered out)"
          exit 0
      
      # Only run these steps if filtering passed
      - name: Validate API key
        if: steps.ultra-filter.outputs.should_process == 'true'
        run: |
          echo "ðŸ”‘ Validating Together API key..."
          if [[ -z "${{ secrets.TOGETHER_API_KEY }}" ]]; then
            echo "::error::Missing Together API key. Please add it to your repository secrets."
            exit 1
          fi
          
          # Validate format without revealing content
          if [[ ! "${{ secrets.TOGETHER_API_KEY }}" =~ ^[A-Za-z0-9+/=_-]+$ ]]; then
            echo "::warning::API key has unexpected format, may cause issues"
          else
            echo "âœ… API key format validation passed"
          fi
      
      - name: Checkout repository
        if: steps.ultra-filter.outputs.should_process == 'true'
        uses: actions/checkout@v4
        
      - name: Set up Node.js
        if: steps.ultra-filter.outputs.should_process == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      
      - name: Initialize workflow logging
        if: steps.ultra-filter.outputs.should_process == 'true'
        id: init
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            // Load logging utilities
            const loggerPath = './.github/shared/utils/logging.js';
            if (!fs.existsSync(loggerPath)) {
              return core.setFailed('Logging utilities not found');
            }
            
            const logger = require(`${process.env.GITHUB_WORKSPACE}/${loggerPath}`);
            const logFile = logger.initLogFile('Issue Assistant', context);
            
            // Start logging
            logger.logSection(logFile, "Workflow Initialization");
            logger.logMessage(logFile, "- Event: " + context.eventName);
            logger.logMessage(logFile, "- Action: " + context.payload.action);
            logger.logMessage(logFile, "- Actor: " + context.actor);
            logger.logMessage(logFile, "- Issue #" + '${{ steps.ultra-filter.outputs.issue_number }}' + ": " + '${{ steps.ultra-filter.outputs.issue_title }}');
            logger.logMessage(logFile, "- Author: " + '${{ steps.ultra-filter.outputs.issue_author }}');
            
            // Make log file available to all steps
            core.setOutput('log_file', logFile);
            
            // Basic validation (already done in ultra-filter, but keeping for completeness)
            if (context.eventName !== 'issues' && context.eventName !== 'issue_comment') {
              logger.logError(logFile, `Unsupported event type: ${context.eventName}`);
              return core.setFailed(`Unsupported event type: ${context.eventName}`);
            }
            
            logger.logSuccess(logFile, 'Workflow initialization completed - proceeding with issue processing');
            return { log_file: logFile };
      
      - name: Extract issue content
        if: steps.ultra-filter.outputs.should_process == 'true'
        id: get-content
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const logFile = '${{ steps.init.outputs.log_file }}';
            
            // Load utilities
            const loggerPath = './.github/shared/utils/logging.js';
            const eventExtractorPath = './.github/shared/event-handlers/event-extractor.js';
            
            if (!fs.existsSync(loggerPath) || !fs.existsSync(eventExtractorPath)) {
              return core.setFailed('Required modules not found');
            }
            
            const logger = require(`${process.env.GITHUB_WORKSPACE}/${loggerPath}`);
            const eventExtractor = require(`${process.env.GITHUB_WORKSPACE}/${eventExtractorPath}`);
            
            logger.logSection(logFile, "Content Extraction");
            
            try {
              // Extract content from issue or comment
              const contentResult = eventExtractor.extractIssueContent(context.payload, logFile, logger);
              
              if (!contentResult.success) {
                logger.logError(logFile, `Failed to extract content: ${contentResult.error}`);
                return core.setFailed(`Failed to extract content: ${contentResult.error}`);
              }
              
              // Get previous comments for context
              let previousComments = [];
              
              if (context.eventName === 'issue_comment' || context.payload.action === 'edited') {
                try {
                  logger.logMessage(logFile, "Fetching previous comments for context");
                  
                  // Use REST API to fetch issue comments
                  const { data: comments } = await github.rest.issues.listComments({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    issue_number: contentResult.issueNumber,
                    per_page: 10
                  });
                  
                  // Format comments for context
                  previousComments = comments.map(comment => ({
                    author: comment.user.login,
                    body: comment.body,
                    createdAt: comment.created_at
                  }));
                  
                  logger.logSuccess(logFile, `Retrieved ${previousComments.length} previous comments`);
                } catch (error) {
                  logger.logWarning(logFile, `Error retrieving previous comments: ${error.message}`);
                  // Continue without previous comments - non-critical error
                }
              }
              
              // Create a JSON file with all extracted content for the next step
              const extractedData = {
                ...contentResult,
                previousComments
              };
              
              fs.writeFileSync('content_data.json', JSON.stringify(extractedData, null, 2));
              
              logger.logSuccess(logFile, "Content extraction completed successfully");
              
              // Explicitly set outputs using core.setOutput
              core.setOutput('issue_id', contentResult.issueId);
              core.setOutput('issue_number', contentResult.issueNumber);
              core.setOutput('comment_id', contentResult.commentId || '');
              core.setOutput('content_file', contentResult.contentFile || '');
              core.setOutput('base64_file', contentResult.base64File || '');
              
              // Also return values for backward compatibility
              return {
                issue_id: contentResult.issueId,
                issue_number: contentResult.issueNumber,
                comment_id: contentResult.commentId,
                content_file: contentResult.contentFile,
                base64_file: contentResult.base64File
              };
            } catch (error) {
              logger.logError(logFile, `Error in content extraction: ${error.message}`);
              return core.setFailed(`Error in content extraction: ${error.message}`);
            }
      
      - name: Analyze repository context needs
        if: steps.ultra-filter.outputs.should_process == 'true'
        id: check-context
        run: |
          echo "Starting repository context analysis..." >> ${{ steps.init.outputs.log_file }}
          
          # Create directories for analysis
          mkdir -p context_analysis error_logs
          
          # Check if content data exists
          if [ ! -f "content_data.json" ]; then
            echo "âŒ Error: content_data.json file not found" >> ${{ steps.init.outputs.log_file }}
            echo "needs_context=false" >> $GITHUB_OUTPUT
            echo "reason=Failed to analyze context needs - missing content data" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Extract content to a temporary file to avoid shell expansion issues
          jq -r '.content' content_data.json > context_analysis/content.txt
          
          # Create the request JSON directly with heredoc
          cat > context_analysis/request.json << EOL
          {
            "model": "deepseek-ai/DeepSeek-R1",
            "messages": [
              {
                "role": "user",
                "content": "You are a tool that analyzes GitHub issue or discussion messages to determine if repository context would be helpful to answer the query. You should be liberal in deciding that context is needed - if there's ANY chance code context would help, say yes. Respond with only a JSON object containing two fields: \"needsContext\" (boolean) and \"reason\" (string).\n\nAnalyze this GitHub issue/discussion text and determine if repository code context would be helpful to answer it properly:\n\n$(cat context_analysis/content.txt)\n\nRespond with JSON only, format: {\"needsContext\": boolean, \"reason\": \"your explanation\"}"
              }
            ]
          }
          EOL
          
          # Call the API
          curl -s -X POST "https://api.together.xyz/v1/chat/completions" \
            -H "Authorization: Bearer ${{ secrets.TOGETHER_API_KEY }}" \
            -H "Content-Type: application/json" \
            -d @context_analysis/request.json \
            -o context_analysis/response.json
          
          # Check if the response file exists and has content
          if [ -s "context_analysis/response.json" ]; then
            # Extract the response content to a file
            jq -r '.choices[0].message.content' context_analysis/response.json > context_analysis/ai_response.txt
            
            # Extract JSON using grep
            grep -o '{.*}' context_analysis/ai_response.txt > context_analysis/json_extract.txt || echo "{}" > context_analysis/json_extract.txt
            
            # Default values
            NEEDS_CONTEXT="false"
            REASON="Failed to analyze context needs"
            
            # Try to extract values from the JSON if file exists and has content
            if [ -s "context_analysis/json_extract.txt" ]; then
              EXTRACTED_NEEDS_CONTEXT=$(jq -r '.needsContext // false' context_analysis/json_extract.txt)
              EXTRACTED_REASON=$(jq -r '.reason // "No reason provided"' context_analysis/json_extract.txt)
              
              if [ "$EXTRACTED_NEEDS_CONTEXT" == "true" ]; then
                NEEDS_CONTEXT="true"
              fi
              
              if [ ! -z "$EXTRACTED_REASON" ] && [ "$EXTRACTED_REASON" != "null" ]; then
                REASON="$EXTRACTED_REASON"
              fi
            fi
            
            echo "Context analysis result: $NEEDS_CONTEXT" >> ${{ steps.init.outputs.log_file }}
            echo "Reason: $REASON" >> ${{ steps.init.outputs.log_file }}
            
            # Set outputs
            echo "needs_context=$NEEDS_CONTEXT" >> $GITHUB_OUTPUT
            echo "reason=$REASON" >> $GITHUB_OUTPUT
          else
            echo "âŒ Error: No response from API or empty response" >> ${{ steps.init.outputs.log_file }}
            # Default to needing context in case of error
            echo "needs_context=true" >> $GITHUB_OUTPUT
            echo "reason=Failed to analyze context needs, assuming context is needed" >> $GITHUB_OUTPUT
          fi
      
      - name: Get repository context
        id: get-repo-context
        if: steps.ultra-filter.outputs.should_process == 'true' && steps.check-context.outputs.needs_context == 'true'
        run: |
          echo "Getting repository context..." >> ${{ steps.init.outputs.log_file }}
          
          # Load content data
          CONTENT=$(cat content_data.json | jq -r '.content')
          
          # We'll use a simpler approach for repo context in the shell script
          # Just identify some relevant files based on keywords in the content
          
          # Extract keywords from content
          KEYWORDS=$(echo "$CONTENT" | tr -cs '[:alnum:]' '\n' | sort | uniq | grep -v '^$' | head -20)
          echo "Keywords extracted: $KEYWORDS" >> ${{ steps.init.outputs.log_file }}
          
          # Find relevant files (limited to 15)
          RELEVANT_FILES=""
          for keyword in $KEYWORDS; do
            if [ ${#keyword} -gt 3 ]; then  # Only use keywords longer than 3 chars
              # First try grep to find content matches (more relevant)
              GREP_FILES=$(grep -l "$keyword" $(find . -type f -not -path "*/node_modules/*" -not -path "*/dist/*" -not -path "*/.git/*" -not -path "*/build/*" -not -path "*/.github/workflows/*" -name "*.js" -o -name "*.jsx" -o -name "*.ts" -o -name "*.tsx" -o -name "*.py" -o -name "*.html" -o -name "*.css" 2>/dev/null) 2>/dev/null | head -3)
              
              # Then try filename matches
              FOUND_FILES=$(find . -type f -not -path "*/node_modules/*" -not -path "*/dist/*" -not -path "*/.git/*" -not -path "*/build/*" -not -path "*/.github/workflows/*" -name "*${keyword}*" | head -2)
              
              RELEVANT_FILES="$RELEVANT_FILES $GREP_FILES $FOUND_FILES"
            fi
          done
          
          # Also include obviously important files
          IMPORTANT_FILES=$(find . -maxdepth 2 -type f -name "package.json" -o -name "README.md" -o -name "index.html" -o -name "index.js" -o -name "main.js" -o -name "app.js" -o -name "styles.css" 2>/dev/null)
          RELEVANT_FILES="$RELEVANT_FILES $IMPORTANT_FILES"
          
          # Deduplicate files and limit to 15
          RELEVANT_FILES=$(echo "$RELEVANT_FILES" | tr ' ' '\n' | sort | uniq | grep -v '^$' | head -15)
          
          # Format the repository context
          echo -e "## Repository Context\n\n" > repo_context.md
          echo -e "### Repository Structure\n" >> repo_context.md
          find . -maxdepth 2 -type d -not -path "*/node_modules/*" -not -path "*/.git/*" | sort | head -10 | sed 's/^/- /' >> repo_context.md
          echo -e "\n\n### Relevant Files\n\n" >> repo_context.md
          
          # Read file contents and add to context
          FILE_COUNT=0
          for file in $RELEVANT_FILES; do
            if [ -f "$file" ]; then
              EXTENSION="${file##*.}"
              echo -e "#### ${file}\n\`\`\`${EXTENSION}" >> repo_context.md
              head -100 "$file" >> repo_context.md
              echo -e "\n\`\`\`\n\n" >> repo_context.md
              FILE_COUNT=$((FILE_COUNT+1))
            fi
          done
          
          echo "Repository context extracted with ${FILE_COUNT} files" >> ${{ steps.init.outputs.log_file }}
          
          # Set output
          echo "context_file=repo_context.md" >> $GITHUB_OUTPUT
          echo "file_count=${FILE_COUNT}" >> $GITHUB_OUTPUT
      
      - name: Generate AI response
        if: steps.ultra-filter.outputs.should_process == 'true'
        id: generate-response
        run: |
          echo "Generating AI response..." >> ${{ steps.init.outputs.log_file }}
          
          # Create response directory
          mkdir -p response_files
          
          # Load content data
          CONTENT_DATA=$(cat content_data.json)
          CONTENT=$(echo "$CONTENT_DATA" | jq -r '.content')
          EVENT_TYPE=$(echo "$CONTENT_DATA" | jq -r '.eventType')
          
          # Load repository context if available
          REPO_CONTEXT=""
          if [[ "${{ steps.check-context.outputs.needs_context }}" == "true" ]] && [[ -f "repo_context.md" ]]; then
            REPO_CONTEXT=$(cat repo_context.md)
            echo "Loaded repository context ($(wc -c < repo_context.md) bytes)" >> ${{ steps.init.outputs.log_file }}
          fi
          
          # Combine system and user prompts into a single user prompt
          cat > response_files/combined_prompt.txt << 'PROMPTEOF'
          You are an AI assistant that helps with GitHub issues. You provide helpful, accurate, and concise responses to technical questions and issues.

          When responding, follow these guidelines:
          1. Be direct and get straight to the point
          2. Use markdown formatting to structure your responses
          3. When code is needed, use proper syntax highlighting with markdown code blocks (e.g., ```javascript)
          4. If you're uncertain, acknowledge limitations rather than making things up
          5. You may include your reasoning process in <think></think> tags - this will be formatted as a quote block in the final response
          6. For complex technical questions, break down your approach step by step
          7. If you reference specific parts of code files, cite the file path and line numbers
          8. Keep your responses focused on the technical issue at hand

          Remember that you're responding in a GitHub issue context. Your goal is to be helpful, accurate, and drive toward issue resolution.
          PROMPTEOF
          
          # Append event type and content
          echo "" >> response_files/combined_prompt.txt
          echo "${EVENT_TYPE}:" >> response_files/combined_prompt.txt
          echo "" >> response_files/combined_prompt.txt
          echo "$CONTENT" >> response_files/combined_prompt.txt
          
          # Add context if available
          if [[ ! -z "$REPO_CONTEXT" ]]; then
            echo "" >> response_files/combined_prompt.txt
            echo "IMPORTANT - Use the following repository context information to assist with your response. Examine this context carefully and refer to specific files and code when relevant to the question:" >> response_files/combined_prompt.txt
            echo "" >> response_files/combined_prompt.txt
            echo "$REPO_CONTEXT" >> response_files/combined_prompt.txt
            echo "" >> response_files/combined_prompt.txt
            echo "Based on the repository context above, please provide a specific and technically accurate response to the question. Reference specific files and code sections when relevant." >> response_files/combined_prompt.txt
          fi
          
          # Create request payload using jq to properly escape JSON
          jq -n \
            --arg prompt "$(cat response_files/combined_prompt.txt)" \
            '{
              "model": "deepseek-ai/DeepSeek-R1",
              "messages": [
                {
                  "role": "user",
                  "content": $prompt
                }
              ]
            }' > response_files/payload.json
          
          # Make API call
          curl -s -X POST "https://api.together.xyz/v1/chat/completions" \
            -H "Authorization: Bearer ${{ secrets.TOGETHER_API_KEY }}" \
            -H "Content-Type: application/json" \
            -d @response_files/payload.json \
            -o response_files/api_response.json
          
          # Check response and extract content
          if [ -s "response_files/api_response.json" ]; then
            # Extract content
            CONTENT=$(cat response_files/api_response.json | jq -r '.choices[0].message.content')
            
            # Process <think> blocks with a simplified approach
            echo "$CONTENT" > response_files/raw_content.txt
            
            # Use awk directly without creating a separate script file
            processed_content=$(awk '
            BEGIN { in_think = 0; output = ""; thinking = ""; }
            
            /<think>/ { in_think = 1; thinking = ""; next; }
            
            /<\/think>/ { 
              in_think = 0;
              if (thinking != "") {
                output = output "\n\n> **Thinking process:**\n";
                split(thinking, lines, "\n");
                for (i in lines) {
                  if (lines[i] != "") output = output "> " lines[i] "\n";
                  else output = output ">\n";
                }
                output = output "\n";
              }
              next;
            }
            
            in_think == 1 { thinking = thinking (thinking=="" ? "" : "\n") $0; next; }
            
            in_think == 0 { output = output (output=="" ? "" : "\n") $0; }
            
            END { print output; }
            ' response_files/raw_content.txt)
            
            # Save processed content
            echo "$processed_content" > response.txt
            echo "Generated AI response successfully ($(wc -c < response.txt) bytes)" >> ${{ steps.init.outputs.log_file }}
            
            # Set output
            echo "response_file=response.txt" >> $GITHUB_OUTPUT
            echo "content_length=$(wc -c < response.txt)" >> $GITHUB_OUTPUT
          else
            echo "âŒ Error: API request failed or returned empty response" >> ${{ steps.init.outputs.log_file }}
            echo "I apologize, but I'm currently experiencing technical difficulties. Please try again later." > response.txt
            echo "response_file=response.txt" >> $GITHUB_OUTPUT
          fi
      
      - name: Post response to issue
        if: steps.ultra-filter.outputs.should_process == 'true'
        id: post-response
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const logFile = '${{ steps.init.outputs.log_file }}';
            
            // Load utilities
            const loggerPath = './.github/shared/utils/logging.js';
            const responseHandlerPath = './.github/shared/response-handlers/github-response.js';
            
            if (!fs.existsSync(loggerPath) || !fs.existsSync(responseHandlerPath)) {
              return core.setFailed('Required modules not found');
            }
            
            const logger = require(`${process.env.GITHUB_WORKSPACE}/${loggerPath}`);
            const responseHandler = require(`${process.env.GITHUB_WORKSPACE}/${responseHandlerPath}`);
            
            logger.logSection(logFile, "Posting Response");
            
            try {
              // Get response content
              const responseFile = 'response.txt';
              if (!fs.existsSync(responseFile)) {
                logger.logError(logFile, "Response file not found");
                return core.setFailed("Response file not found");
              }
              
              const responseContent = fs.readFileSync(responseFile, 'utf8');
              logger.logMessage(logFile, `Loaded response content (${responseContent.length} bytes)`);
              
              // Load issue data from both step outputs and content_data.json
              // The step output is explicitly preferred if available, with content_data.json as fallback
              let issueNumber;
              
              // First try from step output
              const stepOutputIssueNumber = '${{ steps.get-content.outputs.issue_number }}';
              if (stepOutputIssueNumber && stepOutputIssueNumber.trim() !== '') {
                issueNumber = parseInt(stepOutputIssueNumber, 10);
                logger.logMessage(logFile, `Got issue number ${issueNumber} from step output`);
              } 
              // Then try from content_data.json
              else if (fs.existsSync('content_data.json')) {
                try {
                  const contentData = JSON.parse(fs.readFileSync('content_data.json', 'utf8'));
                  issueNumber = contentData.issueNumber;
                  logger.logMessage(logFile, `Got issue number ${issueNumber} from content_data.json`);
                } catch (error) {
                  logger.logWarning(logFile, `Error parsing content_data.json: ${error.message}`);
                }
              }
              // Finally, try to extract from context directly
              if (!issueNumber && context.payload.issue) {
                issueNumber = context.payload.issue.number;
                logger.logMessage(logFile, `Got issue number ${issueNumber} directly from context`);
              }
              
              if (!issueNumber || isNaN(issueNumber)) {
                logger.logError(logFile, "Missing or invalid issue number");
                return core.setFailed("Missing or invalid issue number");
              }
              
              // Post to issue
              const postResult = await responseHandler.postToIssue({
                github,
                issueNumber,
                responseContent,
                logFile,
                logger,
                context
              });
              
              if (!postResult.success) {
                logger.logError(logFile, `Failed to post response: ${postResult.error}`);
                return core.setFailed(`Failed to post response: ${postResult.error}`);
              }
              
              logger.logSuccess(logFile, `Successfully posted response with ID: ${postResult.commentId}`);
              
              return {
                comment_id: postResult.commentId,
                success: true
              };
            } catch (error) {
              logger.logError(logFile, `Error posting response: ${error.message}`);
              return core.setFailed(`Error posting response: ${error.message}`);
            }
      
      - name: Finalize workflow
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            try {
              const logFile = '${{ steps.init.outputs.log_file }}';
              if (!logFile || !fs.existsSync(logFile)) {
                console.log('Log file not found or not specified');
                return;
              }
              
              const loggerPath = './.github/shared/utils/logging.js';
              if (!fs.existsSync(loggerPath)) {
                console.log('Logger module not found');
                return;
              }
              
              const logger = require(`${process.env.GITHUB_WORKSPACE}/${loggerPath}`);
              
              // Create summary of outcomes
              const outcomes = {
                result: '${{ steps.post-response.outcome }}' === 'success' ? 'success' : 'failure',
                eventType: context.eventName,
                action: context.payload.action || 'N/A',
                repoContextUsed: '${{ steps.check-context.outputs.needs_context }}' === 'true',
                filteredOut: '${{ steps.ultra-filter.outputs.should_process }}' !== 'true'
              };
              
              logger.finalizeLog(logFile, context, outcomes);
              console.log(`Workflow log finalized at ${logFile}`);
            } catch (error) {
              console.error(`Error finalizing workflow: ${error.message}`);
            }
      
      - name: Upload workflow logs as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: workflow-logs
          path: |
            workflow_*.log
            content_files/
            context_analysis/
            response_files/
          retention-days: 5 